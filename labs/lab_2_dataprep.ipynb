{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the data\n",
    "college_data = pd.read_csv(\"https://github.com/UVADS/DS-3021/raw/main/data/cc_institution_details.csv\")\n",
    "job_data = pd.read_csv(\"https://raw.githubusercontent.com/DG1606/CMS-R-2020/master/Placement_Data_Full_Class.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1, Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems: \n",
    "This dataset shares information about many colleges, and there are many columns included. We have metrics on class size, graduation percentage, financial information, test scores, and many others. However, a lot of the data is missing. Students may care about how much money they can receive in awards if they are interested in completing research,, but they may not have access to this information. The problem is students not being able to see much and how commonly award money is given and needing to see what information can show how much money schools award.\n",
    "\n",
    "#### Question:\n",
    "Can we predict how much money schools spend on awards from public information?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1, Step 2\n",
    "- Our independent business metric is the amount of money that schools spend on awards as this is what we are looking to forecast.\n",
    "- Taking a look at what information we can easily acquire, we can start with level, private vs. public, student count, median SAT score, graduation percentage, and % of full time students for our predictors. Our target variable is money spent on awards, so we will look at the awards_per_value and exp_award_value, the amount of awards given out per 100 undergraduates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset that includes only the variables of interest\n",
    "college_data_filtered = college_data[['chronname', 'level', 'control', 'student_count', 'med_sat_value', 'grad_100_percentile',\n",
    "                                       'ft_pct', 'exp_award_value', 'awards_per_value']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking structure of our dataset to see if there are any issues\n",
    "college_data_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us explore how many missing values we have: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(college_data_filtered.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we are missing a lot of median SAT value data. There is so much missing data that it seems we are better off dropping it entirely. For the grad_100_percentile, we are missing 300 values. This is a small enough subset of our data that we can drop rows missing grad_100_percentile(and the four missing ft_pct) and not hurt model performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_data_filtered = college_data_filtered[['chronname', 'level', 'control', 'student_count', 'grad_100_percentile',\n",
    "                                       'ft_pct', 'exp_award_value', 'awards_per_value']]\n",
    "college_data_filtered = college_data_filtered.dropna(subset=['grad_100_percentile', 'ft_pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many unique values we have for certain variables that may be categorical\n",
    "print(\"Level classifications:\", college_data_filtered['level'].unique())\n",
    "print(\"Control classifications:\", college_data_filtered['control'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. There are only 4-year and 2-year universities, and only public, private not-for profit and private for-profit.\n",
    "We definitely want to make these categorical variables. \n",
    "We could divide into just public and private, but let's keep further classifications because there are only three"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making level and control into categorical variables\n",
    "college_data_filtered['level'] = college_data_filtered['level'].astype('category')\n",
    "college_data_filtered['control'] = college_data_filtered['control'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of our other variables have distinct values. However, we can collapse to narrow down into broad categories.\n",
    "Specifically, we can divide count of students into levels 1-4, with one being extremely small schools(bottom 25th %), and 4 being very large(top 25%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Boxplot to visualize distribution of size of schools\n",
    "college_data_filtered.boxplot(column= 'student_count', vert= False, grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems like we have a lot of outliers. May be better to just find the percentiles and assign to variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_pct = np.percentile(college_data_filtered['student_count'], [25, 50, 75])\n",
    "count_25 =  count_pct[0]\n",
    "count_50 =  count_pct[1]\n",
    "count_75 =  count_pct[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through colleges to assign values based on size, make categorical variable\n",
    "for index, row in college_data_filtered.iterrows():\n",
    "    if row['student_count'] < count_pct[0]:\n",
    "        college_data_filtered.at[index, 'student_count'] = 1\n",
    "    elif row['student_count'] < count_pct[1]:\n",
    "        college_data_filtered.at[index, 'student_count'] = 2\n",
    "    elif row['student_count'] < count_pct[2]:\n",
    "        college_data_filtered.at[index, 'student_count'] = 3\n",
    "    else:\n",
    "        college_data_filtered.at[index, 'student_count'] = 4\n",
    "\n",
    "college_data_filtered['student_count'] = college_data_filtered['student_count'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we scale our numeric variables, we need to define our target variable. Currently we have normalized values for expected award value and awards per value. This represents how common it is for financial awards to be granted and how much they are for, on average. We can multiply these together to create a metric that factors in both. We want to do this before scaling and normalizing to keep our target variable between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_data_filtered['award_total'] = college_data_filtered['awards_per_value'] * college_data_filtered['exp_award_value']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will scale our numeric variables using min-max scaling, placing all variables between 0 and 1 and making sure all features contribute equally to model performance "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes list of numeric variables, scales them to be between 0 and 1\n",
    "my_list = list(college_data_filtered.select_dtypes(include=['number']))\n",
    "college_data_filtered[my_list] = MinMaxScaler().fit_transform(college_data_filtered[my_list])\n",
    "college_data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our numeric variables now clearly are only between 0 and 1. We have successfully scaled and normalized our numeric variables!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step will be to implement one-hot encoding on our categorical variables, making them into binary variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes a list of categorical variables, sets them to be represented as true or false\n",
    "category_list = list(college_data_filtered.select_dtypes('category'))\n",
    "print(category_list)\n",
    "college_data_filtered_1h = pd.get_dummies(college_data_filtered, columns=category_list)\n",
    "college_data_filtered_1h\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's get an idea of what our target variable looks like/represents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "college_data_filtered_1h.boxplot(column= 'award_total', vert= False, grid=False)\n",
    "college_data_filtered_1h.award_total.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems like a lot of our data is concentrated in lower values. We will define a college with strong funding for awards to be above the 75th percentile, or having an award total metric greater than 0.107"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Assigns award_total in the top 25% to be represented as a binary variable, basically strong or moderate/weak\n",
    "college_data_filtered_1h['award_total'] = pd.cut(college_data_filtered_1h.award_total, bins = [-1,0.107236,1], labels =[0,1])\n",
    "college_data_filtered_1h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking prevalence of our variable\n",
    "prevalence = college_data_filtered_1h.award_total.value_counts()[1]/len(college_data_filtered_1h.award_total)\n",
    "prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is good as we wanted about 1/4 of colleges to be deemed as having good programs for awarding students money"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now drop the variables that were used to calculate our 'award_total' variable as they are no longer useful to us. The target variable is just award total. We also do not need name anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the columns while keeping the same\n",
    "college_dt = college_data_filtered_1h.drop(['exp_award_value', 'awards_per_value', 'chronname'], axis=1)\n",
    "college_dt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can proceed to perform training on our dataset. We will use 25% of the data for training and 75% of the data for testing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = college_dt.shape[0]\n",
    "test_num = round(float(num_rows/4), 0)\n",
    "train_num = num_rows - test_num\n",
    "print(\"Total:\", num_rows)\n",
    "print(\"Training:\", train_num)\n",
    "print(\"Test:\", test_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition data, stratify on award_total to make sure we have even distribution of schools with 1's and 0's in both set\n",
    "Train, Test = train_test_split(college_dt, train_size=2600, stratify=college_dt.award_total, random_state=42)\n",
    "print(Train.shape)\n",
    "print(Test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuning set \n",
    "Tune, Test = train_test_split(Test,  train_size = .5, stratify= Test.award_total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train award total: \", Train.award_total.value_counts())\n",
    "print(671/(1929+671))\n",
    "\n",
    "print(\"Test award total: \", Test.award_total.value_counts())\n",
    "print(112/(112+322))\n",
    "\n",
    "print(\"Tune award total: \", Tune.award_total.value_counts())\n",
    "print(112/(321+112))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our distribution looks good. We have correctly assigned our training and testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do your instincts tell you about the data. Can it address your problem, what areas/items are you worried about? \n",
    "\n",
    "Our data can be used to address our problem. Students who want to ensure they will be financially supported for projects and research can find similarities across universities that provide awards at high values and frequencies. They can see whether public or private institutions provide more,  graduation %, and full time student % are correlated with schools that have strong financial award programs. This will allow students to see more baseline and accessible data to determine whether the university will be a good fit for them or not. As for concerns, it could be a potential issue that we were unable to factor in the schools that contained missing data as this might bias our results. However, since it was a small subset of our dataset it is not a great concern."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 1 Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(award_total\n",
       " 0    1919\n",
       " 1     681\n",
       " Name: count, dtype: int64,\n",
       " award_total\n",
       " 0    320\n",
       " 1    114\n",
       " Name: count, dtype: int64,\n",
       " award_total\n",
       " 0    320\n",
       " 1    113\n",
       " Name: count, dtype: int64,\n",
       "       grad_100_percentile    ft_pct award_total  level_2-year  level_4-year  \\\n",
       " 0                    0.15  0.935551           0         False          True   \n",
       " 1                    0.67  0.716216           1         False          True   \n",
       " 2                    0.00  0.612266           1         False          True   \n",
       " 3                    0.34  0.733888           0         False          True   \n",
       " 4                    0.11  0.906445           0         False          True   \n",
       " ...                   ...       ...         ...           ...           ...   \n",
       " 3793                 0.00  0.319127           0         False          True   \n",
       " 3794                 0.64  0.917879           1         False          True   \n",
       " 3795                 0.82  0.553015           1         False          True   \n",
       " 3796                 0.54  0.689189           1          True         False   \n",
       " 3797                 0.78  0.227651           0         False          True   \n",
       " \n",
       "       control_Private for-profit  control_Private not-for-profit  \\\n",
       " 0                          False                           False   \n",
       " 1                          False                           False   \n",
       " 2                          False                            True   \n",
       " 3                          False                           False   \n",
       " 4                          False                           False   \n",
       " ...                          ...                             ...   \n",
       " 3793                       False                            True   \n",
       " 3794                       False                            True   \n",
       " 3795                        True                           False   \n",
       " 3796                        True                           False   \n",
       " 3797                        True                           False   \n",
       " \n",
       "       control_Public  student_count_1  student_count_2  student_count_3  \\\n",
       " 0               True            False            False             True   \n",
       " 1               True            False            False            False   \n",
       " 2              False             True            False            False   \n",
       " 3               True            False            False            False   \n",
       " 4               True            False            False             True   \n",
       " ...              ...              ...              ...              ...   \n",
       " 3793           False             True            False            False   \n",
       " 3794           False             True            False            False   \n",
       " 3795           False             True            False            False   \n",
       " 3796           False             True            False            False   \n",
       " 3797           False             True            False            False   \n",
       " \n",
       "       student_count_4  \n",
       " 0               False  \n",
       " 1                True  \n",
       " 2               False  \n",
       " 3                True  \n",
       " 4               False  \n",
       " ...               ...  \n",
       " 3793            False  \n",
       " 3794            False  \n",
       " 3795            False  \n",
       " 3796            False  \n",
       " 3797            False  \n",
       " \n",
       " [3467 rows x 12 columns])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def college_data_prep(data):\n",
    "    # Filtering to keep only values we need, dropping NA vals\n",
    "    college_data_filtered = data[['chronname', 'level', 'control', 'student_count', 'grad_100_percentile',\n",
    "                                       'ft_pct', 'exp_award_value', 'awards_per_value']]\n",
    "    college_data_filtered = college_data_filtered.dropna(subset=['grad_100_percentile', 'ft_pct'])\n",
    "    # Making object variables categorical\n",
    "    college_data_filtered['level'] = college_data_filtered['level'].astype('category')\n",
    "    college_data_filtered['control'] = college_data_filtered['control'].astype('category')\n",
    "    \n",
    "    # Making student count categorical\n",
    "    count_pct = np.percentile(college_data_filtered['student_count'], [25, 50, 75])\n",
    "    count_25 =  count_pct[0]\n",
    "    count_50 =  count_pct[1]\n",
    "    count_75 =  count_pct[2]\n",
    "    for index, row in college_data_filtered.iterrows():\n",
    "        if row['student_count'] < count_pct[0]:\n",
    "            college_data_filtered.at[index, 'student_count'] = 1\n",
    "        elif row['student_count'] < count_pct[1]:\n",
    "            college_data_filtered.at[index, 'student_count'] = 2\n",
    "        elif row['student_count'] < count_pct[2]:\n",
    "            college_data_filtered.at[index, 'student_count'] = 3\n",
    "        else:\n",
    "            college_data_filtered.at[index, 'student_count'] = 4\n",
    "    college_data_filtered['student_count'] = college_data_filtered['student_count'].astype('category')\n",
    "    # Defining and adding target variable\n",
    "    college_data_filtered['award_total'] = college_data_filtered['awards_per_value'] * college_data_filtered['exp_award_value']\n",
    "    # Standardizing numeric data\n",
    "    my_list = list(college_data_filtered.select_dtypes(include=['number']))\n",
    "    college_data_filtered[my_list] = MinMaxScaler().fit_transform(college_data_filtered[my_list])\n",
    "    # One hot encoding for categorical variables\n",
    "    category_list = list(college_data_filtered.select_dtypes('category'))\n",
    "    college_data_filtered_1h = pd.get_dummies(college_data_filtered, columns=category_list)\n",
    "    # Redefines target variable as a 0 or 1\n",
    "    college_data_filtered_1h['award_total'] = pd.cut(college_data_filtered_1h.award_total, bins = [-1,0.106659,1], labels =[0,1])\n",
    "    # Drop unneeded columns\n",
    "    college_dt = college_data_filtered_1h.drop(['exp_award_value', 'awards_per_value', 'chronname'], axis=1)\n",
    "    #Partition data into train, test, tune\n",
    "    Train, Test = train_test_split(college_dt, train_size=2600, stratify=college_dt.award_total, random_state=42)\n",
    "    Tune, Test = train_test_split(Test,  train_size = .5, stratify= Test.award_total)\n",
    "    return Train.award_total.value_counts(), Test.award_total.value_counts(), Tune.award_total.value_counts(), college_dt\n",
    "\n",
    "college_data_prep(college_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Problems: \n",
    "This dataset contains information about job placement, with various metrics as well as a job placement and salary variable. This clearly lends itself to the question of what factors most strongly are correlated with salary. I feel there is a lot that can be explored about the relationship between education and salary from this dataset.\n",
    "\n",
    "#### Question:\n",
    "Can we predict salary based on level of education completed as well as performance on exams?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2, Step 2\n",
    "- The business metric we are looking to analyze is salary. The variables we will be looking at will be degree percentage complete, Secondary Education percentage 10th Grade(test scores from 10th grade), Higher Secondary Education percentage - 12th Grade(test scores from 12th grade), and MBA percentage. We will also look at their field of study/specialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a new dataset that includes only the variables of interest. For now we will keep all education related variables.\n",
    "job_data_filtered = job_data[['ssc_p', 'hsc_p', 'degree_p', 'salary', 'hsc_s', 'degree_t', 'specialisation']]\n",
    "job_data_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to make sure that all of our data contains our target variable. It is paramount that we address this issue early on as missing values can cause problems with our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(job_data_filtered.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The only value that is missing is salary. We can drop these rows because this is our target variable and any data without it is not meaningful to us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_filtered = job_data_filtered.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have hsc_s, degree_t, and specialisation as object type variables right now but we may want them to be categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking to see how many unique values there are\n",
    "print(\"hsc_s classifications:\", job_data_filtered['hsc_s'].unique())\n",
    "print(\"degree classifications:\", job_data_filtered['degree_t'].unique())\n",
    "print(\"specialisation classifications:\", job_data_filtered['specialisation'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are only 2-3 unique values for these variables, we can make them into categorical variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_data_filtered['hsc_s'] = job_data_filtered['hsc_s'].astype('category')\n",
    "job_data_filtered['degree_t'] = job_data_filtered['degree_t'].astype('category')\n",
    "job_data_filtered['specialisation'] = job_data_filtered['specialisation'].astype('category')\n",
    "\n",
    "job_data_filtered.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great. Now we have only numeric and categorical variables. Since there are so few classifications we do not need to collapse variables down further. We can now normalize our numeric data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Makes list of numeric variables, scales them to be between 0 and 1\n",
    "job_num_list = list(job_data_filtered.select_dtypes(include=['number']))\n",
    "job_data_filtered[job_num_list] = MinMaxScaler().fit_transform(job_data_filtered[job_num_list])\n",
    "job_data_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our numeric variables have now clearly been scaled to be between 0 and 1. We have successfully scaled our data. Now we will move to one-hot encoding of our categorical variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take our categorical variables and assign them true if present, otherwise false\n",
    "job_cat_list = list(job_data_filtered.select_dtypes('category'))\n",
    "job_data_1h = pd.get_dummies(job_data_filtered, columns=job_cat_list)\n",
    "job_data_1h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have successfully one-hot encoded the data, we can move to defining our target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this problem, since we are looking at factors such as MBA completion and higher education, we want to see how education correlates with high salary. Let us define a high salary as one that is in the top 30th percentile. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dt = job_data_1h\n",
    "job_dt.salary.describe(percentiles=[0.25, 0.50, 0.70, 0.90])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The top 30th percentile is defined as values above 0.1351, so we will represent salary as a binary variable that indicates whether salary is in the top 30th percentile or not"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_dt['salary'] = pd.cut(job_dt.salary, bins = [-1,0.135,1], labels =[0,1])\n",
    "job_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking prevalence of our target variable, high salary\n",
    "sal_prevalence = job_dt.salary.value_counts()[1]/len(job_dt.salary)\n",
    "sal_prevalence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prevalence of our target variable represents about 1/3rd of our data. This is acceptable as our top salary being defined as 1/3 of our data makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating values of training and testing data\n",
    "job_rows = job_dt.shape[0]\n",
    "job_test = round(float(job_rows/4), 0)\n",
    "job_train = job_rows - job_test\n",
    "print(\"Total:\", job_rows)\n",
    "print(\"Training:\", job_train)\n",
    "print(\"Test:\", job_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111, 12)\n",
      "(37, 12)\n"
     ]
    }
   ],
   "source": [
    "# Partitioning data into training and testing, stratifying to ensure top salary data is evenly distributed for both groups\n",
    "# random state used for repeatability\n",
    "job_train, job_test = train_test_split(job_dt, train_size=111, stratify=job_dt.salary, random_state=42)\n",
    "print(job_train.shape)\n",
    "print(job_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tuning set, half of our testing data\n",
    "job_tune, job_test = train_test_split(job_test, train_size = .5, stratify= job_test.salary)\n",
    "print(job_tune.shape)\n",
    "print(job_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train salary total: \", job_train.salary.value_counts())\n",
    "print(38/(73+38))\n",
    "\n",
    "print(\"Test award total: \", job_test.salary.value_counts())\n",
    "print(6/(13+6))\n",
    "\n",
    "print(\"Tune award total: \", job_tune.salary.value_counts())\n",
    "print(6/(12+6))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We aimed to have a prevalence of about 1/3, and our partitions correctly represent that. The distribution is not as exact as with the college data because we are dealing with a smaller dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything looks good. Our function works properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 Step 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What do your instincts tell you about the data. Can it address your problem, what areas/items are you worried about? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset gives us a lot of information that can be used to address our problem, especially our numeric variables. We are able to see what test scores, degree completion, and level of education correlate with a high salary. Our categorical variables allow for stratification across fields so we can see how these metrics vary across specialization. We would likely need to build a regression model that shows the correlation between the variables and salary. From this, we could define cutoff points for what test scores and level of education are generally needed to achieve a high salary. This would address our problem as people could use this information to determine how much time and money they need to invest in their education in order for them to have a high salary. A concern of mine would be the dataset is not large enough as we only have slightly over 100 observations. For us to draw more generalizable conclusions, a larger dataset would be preferred. There is great variance in both education and jobs so we want to ensure the general public is well represented. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset 2 Step 4\n",
    "#### Function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_3910/4071431339.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  job_data_filtered['hsc_s'] = job_data_filtered['hsc_s'].astype('category')\n",
      "/tmp/ipykernel_3910/4071431339.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  job_data_filtered['degree_t'] = job_data_filtered['degree_t'].astype('category')\n",
      "/tmp/ipykernel_3910/4071431339.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  job_data_filtered['specialisation'] = job_data_filtered['specialisation'].astype('category')\n",
      "/tmp/ipykernel_3910/4071431339.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  job_data_filtered[job_num_list] = MinMaxScaler().fit_transform(job_data_filtered[job_num_list])\n",
      "/tmp/ipykernel_3910/4071431339.py:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  job_dt['salary'] = pd.cut(job_dt.salary, bins = [-1,0.135,1], labels =[0,1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(salary\n",
       " 0    73\n",
       " 1    38\n",
       " Name: count, dtype: int64,\n",
       " salary\n",
       " 0    13\n",
       " 1     6\n",
       " Name: count, dtype: int64,\n",
       " salary\n",
       " 0    12\n",
       " 1     6\n",
       " Name: count, dtype: int64,\n",
       "         ssc_p     hsc_p  degree_p salary  hsc_s_Arts  hsc_s_Commerce  \\\n",
       " 0    0.538240  0.889621  0.195122      0       False            True   \n",
       " 1    0.792414  0.680890  0.670244      0       False           False   \n",
       " 2    0.497011  0.510708  0.341463      0        True           False   \n",
       " 4    0.925788  0.602965  0.568293      1       False            True   \n",
       " 7    0.847454  0.444811  0.390244      0       False           False   \n",
       " ..        ...       ...       ...    ...         ...             ...   \n",
       " 209  0.435168  0.576606  0.365854      0       False            True   \n",
       " 210  0.818594  0.741351  0.673171      1       False            True   \n",
       " 211  0.352711  0.378913  0.536585      0       False           False   \n",
       " 212  0.538240  0.494234  0.560976      0       False            True   \n",
       " 213  0.682540  0.477759  0.195122      0       False            True   \n",
       " \n",
       "      hsc_s_Science  degree_t_Comm&Mgmt  degree_t_Others  degree_t_Sci&Tech  \\\n",
       " 0            False               False            False               True   \n",
       " 1             True               False            False               True   \n",
       " 2            False                True            False              False   \n",
       " 4            False                True            False              False   \n",
       " 7             True               False            False               True   \n",
       " ..             ...                 ...              ...                ...   \n",
       " 209          False                True            False              False   \n",
       " 210          False                True            False              False   \n",
       " 211           True               False            False               True   \n",
       " 212          False                True            False              False   \n",
       " 213          False                True            False              False   \n",
       " \n",
       "      specialisation_Mkt&Fin  specialisation_Mkt&HR  \n",
       " 0                     False                   True  \n",
       " 1                      True                  False  \n",
       " 2                      True                  False  \n",
       " 4                      True                  False  \n",
       " 7                      True                  False  \n",
       " ..                      ...                    ...  \n",
       " 209                    True                  False  \n",
       " 210                    True                  False  \n",
       " 211                    True                  False  \n",
       " 212                    True                  False  \n",
       " 213                   False                   True  \n",
       " \n",
       " [148 rows x 12 columns])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def job_data_prep(data):\n",
    "    # Selecting our variables of interest\n",
    "    job_data_filtered = data[['ssc_p', 'hsc_p', 'degree_p', 'salary', 'hsc_s', 'degree_t', 'specialisation']]\n",
    "    # Making object variables categorical\n",
    "    job_data_filtered['hsc_s'] = job_data_filtered['hsc_s'].astype('category')\n",
    "    job_data_filtered['degree_t'] = job_data_filtered['degree_t'].astype('category')\n",
    "    job_data_filtered['specialisation'] = job_data_filtered['specialisation'].astype('category')\n",
    "    # Scaling/normalizing our numeric data\n",
    "    job_num_list = list(job_data_filtered.select_dtypes(include=['number']))\n",
    "    job_data_filtered[job_num_list] = MinMaxScaler().fit_transform(job_data_filtered[job_num_list])\n",
    "    # One hot encoding \n",
    "    job_cat_list = list(job_data_filtered.select_dtypes('category'))\n",
    "    job_data_1h = pd.get_dummies(job_data_filtered, columns=job_cat_list)\n",
    "    # Dropping data that doesn't have salary information\n",
    "    job_dt = job_data_1h.dropna(subset=['salary'])\n",
    "    # Making our target variable binary\n",
    "    job_dt['salary'] = pd.cut(job_dt.salary, bins = [-1,0.135,1], labels =[0,1])\n",
    "    # Partitioning our data\n",
    "    job_train, job_test = train_test_split(job_dt, train_size=111, stratify=job_dt.salary, random_state=42)\n",
    "    job_tune, job_test = train_test_split(job_test, train_size = .5, stratify= job_test.salary)\n",
    "    return job_train.salary.value_counts(), job_test.salary.value_counts(), job_tune.salary.value_counts(), job_dt\n",
    "\n",
    "job_data_prep(job_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
